# VisionTransformer-MNIST

## An attention map for a test image ([code](https://github.com/mashaan14/VisionTransformer-MNIST/blob/main/VisionTransformer_MNIST.ipynb)) ([video](https://youtu.be/k1XSbJOq824?feature=shared))
This notebook is designed to plot the attention maps of a vision transformer trained on MNIST digits. Looking at the attached gif, the neural net knows where to “pay attention”.

I’ve looked at multiple resources, but these two were particularly useful:
  - [TUTORIAL 11: VISION TRANSFORMERS](https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/11-vision-transformer.html)
  - [An unofficial colab walkthrough of Vision Transformer](https://medium.com/@hirotoschwert/an-unofficial-colab-walkthrough-of-vision-transformer-8bcd592ba26a)
  - 

<p align="center">
  <img src="myimage.gif" />
</p>

## An attention map for query and test images ([code](https://github.com/mashaan14/VisionTransformer-MNIST/blob/main/VisionTransformer_MNIST_query_key.ipynb)) ([video](https://youtube.com/shorts/377b_Z_PjzE?feature=shared))
Given query and key images, the code here will plot an attention map. I used MNIST digits dataset.

I borrowed the transformer code from this link:
  - [TUTORIAL 11: VISION TRANSFORMERS](https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/11-vision-transformer.html)

<p align="center">
  <img src="myimage1.gif" />
</p>

